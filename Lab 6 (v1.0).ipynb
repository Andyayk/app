{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS450 - Text Mining and Language Processing\n",
    "\n",
    "## Lab 6\n",
    "\n",
    "### Objectives \n",
    "\n",
    "-   To learn to define functions in Python.\n",
    "\n",
    "-   To learn to write Python scripts and import them. Explore two python programs\n",
    "\n",
    "    `1. preprocess.py`\n",
    "\n",
    "    `2. k_means.py`\n",
    "\n",
    "-   To perform K-means document clustering using a provided\n",
    "    Python module, k_means.py.\n",
    "\n",
    "### More on Python Programming\n",
    "\n",
    "In this section, we are going to learn a bit more basic Python\n",
    "programming concepts so that we can re-use Python code. First of all,\n",
    "let us visit the concept of *function*. If you have programming\n",
    "background, then *function* is not new to you. Depending on the\n",
    "programming language(s) you have used before, functions may also be\n",
    "referred to as *methods* (e.g., in Java) or *subroutines* (e.g., in Perl).\n",
    "Even if you have not programmed before, for those of you familiar with\n",
    "Microsoft Excel, you probably have used pre-defined functions in Excel\n",
    "such as `SUM` and `AVERAGE` to help data processing. Generally speaking,\n",
    "a function takes in a list of parameters and performs some computation\n",
    "based on the parameters. A function may or may not return a value.\n",
    "\n",
    "We have been calling many functions in Python so far. For example, we can\n",
    "use `len(lst)` to obtain the length of a list `lst`. Here `len` is a\n",
    "function that takes in a list object and returns its length. We have\n",
    "also used the function `print` to display an object without any returned\n",
    "value.\n",
    "\n",
    "Let us now see how we can define our own function. In\n",
    "the code below, a function called `square` is defined. We can see\n",
    "that to define a function, we need to use the keyword `def` followed by\n",
    "a pair of round brackets `()`. Inside the round brackets, we need to\n",
    "specify the parameters of this function. After the closing round\n",
    "bracket, we need a colon `:`.\n",
    "\n",
    "See the code below for the defintion of a function called `square()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the line below the `def` statement, we need to give the\n",
    "definition of the function. \n",
    "Pay attend to the extra space in front of the `return` statement in\n",
    "the code below. Because Python relies on code indentation to\n",
    "interpret the code, it is important to properly indent your code when\n",
    "necessary. Here we need the extra space before `return` because the\n",
    "`return` statement is part of the `square` function. Once a function is\n",
    "defined, it can be called as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "y = square(4)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible for us to define a function in a separate Python file and `import` the file or the function for us to re-use the function.\n",
    "The Python file containing the function definition is essentially a text file with the file extension `.py`. \n",
    "\n",
    "\n",
    "For example, check the file `lab6_demo.py` that you should have downloaded together with this Jupyter notebook.\n",
    "You can open the file using Notepad++.\n",
    "\n",
    "After examine `lab6_demo.py`, you can now import this file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lab6_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call the function `my_square()` defined in `lab6_demo.py` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.25\n"
     ]
    }
   ],
   "source": [
    "y = lab6_demo.my_square(3.5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, you can place any Python code inside a `.py` file.\n",
    "This is called a Python module.\n",
    "You can then import this Python module to use the functions defined inside.\n",
    "A Python module can also contain variables that can be re-used.\n",
    "See the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14\n"
     ]
    }
   ],
   "source": [
    "print(lab6_demo.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you import a module, you need to use the name of the module\n",
    "together with a function name or variable name to refer to that function\n",
    "or variable. If you need to frequently refer to a\n",
    "function or a variable from a module, you can directly import that\n",
    "function or variable from the module, as shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lab6_demo import my_square\n",
    "print(my_square(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing using a Module\n",
    "\n",
    "Examine another script `preprocess.py` that comes together with Lab 5. Study the functions defined inside. Think about how you can use these functions to pre-process a corpus.\n",
    "\n",
    "You can try to use the functions inside `preprocess.py` to process the text files inside `SGNewsForClustering`, which you can download from eLearn.\n",
    "\n",
    "In the next section, we will illustrate how we can use `preprocess.py` to process these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering using K-Means\n",
    "\n",
    "\n",
    "During the lecture we have explained the basic idea behind the K-means clustering\n",
    "algorithm. In this section, we will test out this algorithm. A Python\n",
    "module called `k_means` has been written for you together with Lab 6. \n",
    "If you are interested, you can take a look at how K-means is\n",
    "implemented in this script, but you are not required to understand\n",
    "everything in the script.\n",
    "\n",
    "You can compare the code with the algorithm described in http://www.onmyphd.com/?p=k-means.clustering.\n",
    "\n",
    "Now let us see how we can use the function `k_means` defined in this\n",
    "module to perform K-means text clustering. To begin with, we need to load a\n",
    "corpus that contains documents. A corpus called `SGNewsForClustering` has been prepared for you. Download the zipped\n",
    "corpus from eLearn. Inside the directory `SGNewsForClustering`, you will\n",
    "see 40 documents. Currently the documents are named after the category\n",
    "they belong to. However, when we load the corpus, the category\n",
    "information is not used. We will see how based on only the content of\n",
    "these documents we can cluster them into two groups and the two groups\n",
    "more or less correspond to the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the module `preprocess` to load\n",
    "this corpus and transform the documents into TF-IDF-based document\n",
    "vectors. Notice that here we use `from preprocess import *` to import\n",
    "all the variables and functions from `preprocess.py`. Then we can\n",
    "directly use the names of those variables and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "\n",
    "corpus = load_corpus('data/SGNewsForClustering')\n",
    "\n",
    "docs = corpus2docs(corpus)\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "vecs = docs2vecs(docs, dictionary)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the `k_means` module and call the function `k_means` to\n",
    "cluster the documents represented by `vecs`. To do so, we need to pass\n",
    "three parameters to `k_means`. The first is `vecs` itself. The second is\n",
    "the dimension of these vectors, which is the same as the number of\n",
    "unique words contained in `dictionary`. The last parameter is the number\n",
    "of clusters we want to generate.\n",
    "\n",
    "\n",
    "The code below shows how we call the `k_means` function to do\n",
    "clustering. We set the number of clusters\n",
    "to be 2. What is returned is a list where each element represents a\n",
    "cluster of documents. A cluster of documents is represented by the\n",
    "indices of the documents in the original document vector list passed to\n",
    "`k_means`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import k_means\n",
    "\n",
    "num_tokens = len(dictionary.token2id)\n",
    "clusters = k_means.k_means(vecs, num_tokens, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how we can take out the first\n",
    "cluster and the second cluster, and display the original file IDs of the documents belonging to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['SGNewsForClustering/Crime_9843.txt', 'SGNewsForClustering/Crime_9844.txt', 'SGNewsForClustering/Crime_9845.txt', 'SGNewsForClustering/Crime_9846.txt', 'SGNewsForClustering/Crime_9847.txt', 'SGNewsForClustering/Crime_9863.txt', 'SGNewsForClustering/Crime_9864.txt', 'SGNewsForClustering/Crime_9865.txt', 'SGNewsForClustering/Crime_9866.txt', 'SGNewsForClustering/Crime_9867.txt', 'SGNewsForClustering/Crime_9868.txt', 'SGNewsForClustering/Crime_9873.txt', 'SGNewsForClustering/Crime_9929.txt', 'SGNewsForClustering/Crime_9930.txt', 'SGNewsForClustering/Crime_9931.txt', 'SGNewsForClustering/Crime_9939.txt', 'SGNewsForClustering/Crime_9940.txt', 'SGNewsForClustering/Crime_9946.txt', 'SGNewsForClustering/Science_9201.txt', 'SGNewsForClustering/Science_9265.txt', 'SGNewsForClustering/Science_9323.txt']\n",
      "Cluster 2: ['SGNewsForClustering/Crime_9842.txt', 'SGNewsForClustering/Crime_9890.txt', 'SGNewsForClustering/Science_9060.txt', 'SGNewsForClustering/Science_9165.txt', 'SGNewsForClustering/Science_9173.txt', 'SGNewsForClustering/Science_9202.txt', 'SGNewsForClustering/Science_9211.txt', 'SGNewsForClustering/Science_9223.txt', 'SGNewsForClustering/Science_9256.txt', 'SGNewsForClustering/Science_9257.txt', 'SGNewsForClustering/Science_9266.txt', 'SGNewsForClustering/Science_9267.txt', 'SGNewsForClustering/Science_9268.txt', 'SGNewsForClustering/Science_9308.txt', 'SGNewsForClustering/Science_9312.txt', 'SGNewsForClustering/Science_9313.txt', 'SGNewsForClustering/Science_9314.txt', 'SGNewsForClustering/Science_9340.txt', 'SGNewsForClustering/Science_9341.txt']\n"
     ]
    }
   ],
   "source": [
    "fids = corpus.fileids()\n",
    "\n",
    "#The below prints the file ids in each cluster\n",
    "\n",
    "cluster1 = clusters[0]\n",
    "print(\"Cluster 1:\", [fids[d] for d in cluster1])\n",
    "\n",
    "cluster2 = clusters[1]\n",
    "print(\"Cluster 2:\", [fids[d] for d in cluster2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if most of the files in the same cluster indeed come from the same category (either Crime or Science)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because in the implementation of the K-means clustering\n",
    "algorithm, we randomly initialize the cluster centers, the result\n",
    "clusters are not always the same. You may see a different list of\n",
    "documents in your first cluster. If you call `k_means` again to cluster\n",
    "the same set of documents, you may get very different results from the\n",
    "first run. All these are normal and nothing to be concerned.\n",
    "\n",
    "What you would like to check is the quality of the clustering results.\n",
    "Although there is no universal truth about what result is considered\n",
    "correct, we can roughly use the documents’ category labels to judge the\n",
    "clustering quality. You can repeat the call to `k_means` several times and\n",
    "check the names of the documents assigned to each cluster. Do you\n",
    "usually see Crime documents grouped together in one cluster and Science\n",
    "documents grouped together in another cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Clusters \n",
    "\n",
    "Generally we cluster documents for which we do not have any category\n",
    "information. In this case, how do we check the quality of the generated\n",
    "clusters? Or a related question is how we can interpret or summarize the\n",
    "generated document clusters. A simple way is to find the most frequent\n",
    "words of a document cluster as an overview of that cluster. Using the\n",
    "two document clusters you have obtained in the previous section, can you\n",
    "use NLTK’s `FreqDist` class to check the most frequent words of each\n",
    "document cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('police', 37), ('said', 33), ('arrested', 20), ('found', 20), ('suspect', 20), ('year', 18), ('also', 17), ('june', 16), ('investigations', 14), ('years', 14)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "\n",
    "# Take all the file IDs in cluster1\n",
    "cluster1_fids = [fids[d] for d in cluster1]\n",
    "\n",
    "# Create an empty list clust1_words =[]\n",
    "cluster1_all_words = []\n",
    "\n",
    "# Add the words from all files to this list. Use corpus.words and extend method\n",
    "for fid in cluster1_fids:\n",
    "    cluster1_all_words.extend(corpus.words(fid))\n",
    "\n",
    "    \n",
    "#\n",
    "# Remove the Stopwords from this list\n",
    "\n",
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_words1 = [w.lower() for w in cluster1_all_words]\n",
    "all_words2 = [w for w in all_words1 if w not in stop_list and len(w)>3]\n",
    "\n",
    "#Call freq distribution metod and display top 10 words or 20 words\n",
    "fdist = nltk.FreqDist(all_words2)\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
